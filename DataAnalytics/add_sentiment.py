# –≥–∏–±—Ä–∏–¥ –±–µ–∑ –Ω–µ–π—Ä–æ–Ω–∫–∏ 
# —Å–ª–æ–≤–∞—Ä—å RuSentiLex + —Å–≤–æ–π —Å–ª–æ–≤–∞—Ä—å -> –Ω–∞–∏–≤–Ω—ã–π –±–∞–µ—Å -> –∞–Ω—Å–∞–º–±–ª—å —Å –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–π –∑–æ–Ω–æ–π 

import csv, sys, re, math
from pathlib import Path
from collections import Counter

DEFAULT_CSV = "Csv/Reviews/all_reviews.csv"
TEXT_COL = "text"
SENT_COL = "sentiment"
RATING_COL = "rating"   # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¢–û–õ–¨–ö–û –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ë–∞–π–µ—Å–∞ (>=4 ‚Üí pos, <=2 ‚Üí neg)

# ---------- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è ----------
import pymorphy3 as pymorphy
_morph = pymorphy.MorphAnalyzer()

def lemma(tok: str) -> str:
    t = (tok or "").lower().replace("—ë", "–µ")
    if _morph:
        try:
            return _morph.parse(t)[0].normal_form
        except Exception:
            return t
    return t

# ---------- –ª–µ–∫—Å–∏–∫–∞ (–ª–æ–∫–∞–ª—å–Ω—ã–π RuSentiLex ‚Äî –µ—Å–ª–∏ –ø–æ–ª–æ–∂–∏—Ç–µ —Ñ–∞–π–ª —Ä—è–¥–æ–º) ----------
def load_local_rusentilex() -> dict:
    candidates = [
        Path("lexicons/rusentilex_2017.txt"),
        Path("lexicons/rusentilex_2017.tsv"),
        Path("lexicons/rusentilex_2017.csv"),
        Path("rusentilex_2017.txt"),
        Path("rusentilex.txt"),
    ]
    file = next((p for p in candidates if p.exists()), None)
    if not file:
        return {}
    lex = {}
    with file.open("r", encoding="utf-8", errors="ignore") as f:
        for raw in f:
            s = raw.strip()
            if not s or s.startswith("#"):
                continue
            parts = re.split(r"\t|;|,|\s{2,}", s)
            w = None
            for x in parts[1:]:
                m = re.search(r"[-+]?\d+(?:\.\d+)?", x)
                if m:
                    try:
                        w = float(m.group(0)); break
                    except:
                        pass
            if w is None:
                j = " ".join(parts[1:]).lower()
                if "pos" in j: w = 1.0
                elif "neg" in j: w = -1.0
            if w is None: 
                continue
            w = float(w)
            lem = lemma((parts[0] if parts else "").strip())
            if lem:
                old = lex.get(lem)
                if old is None or abs(w) > abs(old):
                    lex[lem] = w
    return lex

RU_SENTI = load_local_rusentilex()

# –∑–∞–ø–∞—Å–Ω–æ–π –º–∞–ª–µ–Ω—å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å (–µ—Å–ª–∏ –±–æ–ª—å—à–æ–≥–æ –Ω–µ—Ç)
POS_BASE = {
    "—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å","—Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å","–º–æ–ª–æ–¥–µ—Ü","–æ—Ç–ª–∏—á–Ω–æ","–æ—Ç–ª–∏—á–Ω—ã–π","—Ö–æ—Ä–æ—à–æ","—Ö–æ—Ä–æ—à–∏–π","—Å—É–ø–µ—Ä","–∫–ª–∞—Å—Å–Ω—ã–π",
    "–±—ã—Å—Ç—Ä–æ","–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ","–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ","–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ","–∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π","–¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π","–≤–µ–∂–ª–∏–≤—ã–π",
    "–ø—Ä–∏–≤–µ—Ç–ª–∏–≤—ã–π","—É–¥–æ–±–Ω–æ","–ø–æ–Ω—Ä–∞–≤–∏—Ç—å—Å—è","—Å–ø–∞—Å–∏–±–æ","–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å","–ª—É—á—à–∏–π","–¥–æ–≤–æ–ª—å–Ω—ã–π","–∞–∫–∫—É—Ä–∞—Ç–Ω–æ",
    "–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã–π","—á–µ—Å—Ç–Ω—ã–π","—á–∏—Å—Ç–æ","–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π","–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π","—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π"
}
NEG_BASE = {
    "–ø–ª–æ—Ö–æ","—É–∂–∞—Å–Ω–æ","—É–∂–∞—Å","–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ","–º–µ–¥–ª–µ–Ω–Ω–æ","–¥–æ—Ä–æ–≥–æ","–æ–±–º–∞–Ω","—Ä–∞–∑–≤–æ–¥","—Ö–∞–º—Å—Ç–≤–æ","—Ö–∞–º",
    "–≥—Ä—É–±–æ—Å—Ç—å","–≥—Ä—É–±–æ","–Ω–µ–ø—Ä–∏—è—Ç–Ω–æ","–Ω–µ–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã–π","–∫–æ—Å—è–∫","–ø—Ä–æ–±–ª–µ–º–∞","–∫–æ—à–º–∞—Ä","—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ",
    "–∑–∞–¥–µ—Ä–∂–∫–∞","–æ–±–º–∞–Ω—É—Ç—å","–≤—Ä–∞—Ç—å","–ª–≥–∞—Ç—å","–Ω–∞–ø–ª–µ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π","–Ω–µ–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ","—Ö—É–¥—à–∏–π","–≥—Ä—è–∑–Ω–æ",
    "–æ—Ç—Å—Ç–æ–π","–∂—É—Ç—å","–ª–æ—Ö–æ—Ç—Ä–æ–Ω","—Å–∫–æ—Ç—Å–∫–∏–π","–º–µ—Ä–∑–∫–∏–π","–º–æ—à–µ–Ω–Ω–∏–∫","–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ","–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ","–æ—Ç–≤—Ä–∞—Ç–Ω–æ"
}

NEGATIONS     = {"–Ω–µ","–Ω–µ—Ç","–Ω–∏","–Ω–∏–∫–æ–≥–¥–∞","–Ω–∏—á—É—Ç—å","–Ω–∏–∫–∞–∫","–±–µ–∑"}
INTENSIFIERS  = {"–æ—á–µ–Ω—å","–∫—Ä–∞–π–Ω–µ","—Å—É–ø–µ—Ä","—Ä–µ–∞–ª—å–Ω–æ","–ø—Ä—è–º","–±–µ–∑—É–º–Ω–æ","—á–µ—Ä—Ç–æ–≤—Å–∫–∏"}
DIMINISHERS   = {"—Å–ª–µ–≥–∫–∞","–Ω–µ–º–Ω–æ–≥–æ","—á—É—Ç—å","—á—É—Ç–∫–∞","–µ–¥–≤–∞","–Ω–µ–±–æ–ª—å—à–æ"}
POS_EMOJI     = {"üôÇ","üòä","üòÉ","üòç","üëç","üî•","üí™","ü•≥","üòÑ","üòé"}
NEG_EMOJI     = {"‚òπ","üôÅ","üò†","üò°","üëé","üí©","üò≠","üò§","üòû","üò£"}
# --- —Å—Ç–∞–ª–æ (—É–±—Ä–∞–ª–∏ ¬´–æ–±–º–∞–Ω¬ª –∫–∞–∫ —Ç—Ä–∏–≥–≥–µ—Ä, –æ—Å—Ç–∞–≤–∏–ª–∏ –µ–≥–æ –≤ –ª–µ–∫—Å–∏–∫–æ–Ω–µ) ---
NEG_TRIGGERS  = {"–Ω–µ —Å–æ–≤–µ—Ç—É—é","–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é","–Ω–∏ –≤ –∫–æ–µ–º —Å–ª—É—á–∞–µ","–Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å",
                 "–ø–æ–ª–Ω—ã–π —É–∂–∞—Å","–ø–æ–ª–Ω—ã–π –æ—Ç—Å—Ç–æ–π","—Ä–∞–∑–≤–æ–¥","–ª–æ—Ö–æ—Ç—Ä–æ–Ω","–º–æ—à–µ–Ω–Ω–∏–∫","–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ","–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ","–æ—Ç–≤—Ä–∞—Ç–Ω–æ"}
POS_TRIGGERS  = {"–±–µ–∑ –ø—Ä–æ–±–ª–µ–º","–≤—Å–µ –æ—Ç–ª–∏—á–Ω–æ","–≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ","–æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω","–æ—á–µ–Ω—å –¥–æ–≤–æ–ª—å–Ω–∞",
                 "–≤—Å–µ —Å—É–ø–µ—Ä","–≤—Å—ë —Å—É–ø–µ—Ä"}

# —É—Å–∏–ª–∏—Ç–µ–ª–∏/–ø–æ—Ä–æ–≥–∏ (–ª–µ–∫—Å–∏–∫–∞)
NEG_WEIGHT_MULT       = 1.6
NEG_INVERT_POS_MULT   = 1.5
LEX_POS_TH_SHORT      = 0.12
LEX_NEG_TH_SHORT      = -0.10
LEX_POS_TH_LONG       = 0.14
LEX_NEG_TH_LONG       = -0.11

# ---------- –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å ----------
def normalize_text(s: str) -> str:
    s = (s or "").lower().replace("—ë","–µ")
    s = re.sub(r"[^\w\s!?%:+\-]", " ", s, flags=re.U)
    return re.sub(r"\s+", " ", s).strip()

def to_tokens(s: str):
    return [t for t in re.split(r"\s+", s) if t]

def emoji_score(raw: str) -> float:
    p = sum(ch in POS_EMOJI for ch in (raw or ""))
    n = sum(ch in NEG_EMOJI for ch in (raw or ""))
    return float(p - n)

def phrase_flags_and_score(raw: str):
    txt = (raw or "").lower().replace("—ë","–µ")
    toks = re.findall(r"\w+|[!?]+", txt)

    negations = {"–Ω–µ","–±–µ–∑","–Ω–∏"}
    hard_neg = False
    hard_pos = False
    ph_score = 0.0

    # –Ø–≤–Ω—ã–µ –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã ‚Äî –∫–∞–∫ –µ—Å—Ç—å
    multi_neg = ["–Ω–µ —Å–æ–≤–µ—Ç—É—é","–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é","–Ω–∏ –≤ –∫–æ–µ–º —Å–ª—É—á–∞–µ","–Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å","–ø–æ–ª–Ω—ã–π —É–∂–∞—Å","–ø–æ–ª–Ω—ã–π –æ—Ç—Å—Ç–æ–π"]
    for ph in multi_neg:
        if ph in txt:
            hard_neg = True

    # –û–¥–Ω–æ—Å–ª–æ–≤–Ω—ã–µ —Å–∏–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏, –ù–û —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç ¬´–±–µ–∑/–Ω–µ/–Ω–∏¬ª –ø–µ—Ä–µ–¥ —Å–ª–æ–≤–æ–º
    single_neg = ["—Ä–∞–∑–≤–æ–¥","–ª–æ—Ö–æ—Ç—Ä–æ–Ω","–º–æ—à–µ–Ω–Ω–∏–∫","–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ"]
    for i, w in enumerate(toks):
        for trg in single_neg:
            if w.startswith(trg):
                prev = {toks[j] for j in (i-1, i-2) if j >= 0}
                if prev.isdisjoint(negations):  # –Ω–µ—Ç –æ—Ç—Ä–∏—Ü–∞–Ω–∏—è —Ä—è–¥–æ–º
                    hard_neg = True

    # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
    for ph in ["–±–µ–∑ –ø—Ä–æ–±–ª–µ–º","–≤—Å–µ –æ—Ç–ª–∏—á–Ω–æ","–≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ","–æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω","–æ—á–µ–Ω—å –¥–æ–≤–æ–ª—å–Ω–∞","–≤—Å–µ —Å—É–ø–µ—Ä","–≤—Å—ë —Å—É–ø–µ—Ä"]:
        if ph in txt:
            hard_pos = True

    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ ¬´–±–µ–∑ –æ–±–º–∞–Ω–∞¬ª (—á–∞—Å—Ç—ã–π —à–∞–±–ª–æ–Ω)
    if re.search(r"\b–±–µ–∑\s+–æ–±–º–∞–Ω\w*\b", txt):
        ph_score += 1.5

    return hard_neg, hard_pos, ph_score


# ---------- –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π —Å–∫–æ—Ä ----------
def lex_word_weight(tok: str) -> float:
    lem = lemma(tok)
    if RU_SENTI:
        w = RU_SENTI.get(lem)
        if w is not None:
            return float(w * NEG_WEIGHT_MULT) if w < 0 else float(w)
    if lem in POS_BASE: return 1.0
    if lem in NEG_BASE: return -1.0 * NEG_WEIGHT_MULT
    return 0.0

def lex_score(text: str):
    if not text: return 0.0, 0, 0, 0
    raw = text
    toks = to_tokens(normalize_text(text))
    score = emoji_score(raw)
    pos_hits = 0; neg_hits = 0

    for i, tok in enumerate(toks):
        base = lex_word_weight(tok)
        if base == 0.0: 
            continue
        negated = any(i-k >= 0 and toks[i-k] in NEGATIONS for k in (1,2))
        w = base
        if negated:
            w = -base * (NEG_INVERT_POS_MULT if base > 0 else 1.0)
        for k in (1,2):
            j = i - k
            if j >= 0:
                if toks[j] in INTENSIFIERS: w *= 1.4
                elif toks[j] in DIMINISHERS: w *= 0.65
        score += w

        if (base > 0 and not negated) or (base < 0 and negated):
            pos_hits += 1
        elif (base < 0 and not negated) or (base > 0 and negated):
            neg_hits += 1

    if score != 0:
        score *= 1.0 + min(raw.count("!"), 3)*0.1
    return score, len(toks), pos_hits, neg_hits

def lex_label(score: float, n: int) -> str:
    denom = max(3.0, float(n))
    norm = score / denom
    if n <= 5:
        if norm > LEX_POS_TH_SHORT: return "positive"
        if norm < LEX_NEG_TH_SHORT: return "negative"
    else:
        if norm > LEX_POS_TH_LONG:  return "positive"
        if norm < LEX_NEG_TH_LONG:  return "negative"
    return "neutral"

# ---------- –ù–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å (–∏–∑ —Ç–µ–∫—Å—Ç–∞ CSV) ----------
class NBModel:
    def __init__(self):
        self.pos_counts = Counter()
        self.neg_counts = Counter()
        self.pos_total  = 0
        self.neg_total  = 0
        self.vocab      = set()
        self.pos_docs   = 0
        self.neg_docs   = 0

    def fit_doc(self, text: str, label: str):
        toks = [lemma(t) for t in to_tokens(normalize_text(text))]
        if not toks: return
        for w in toks:
            if label == "pos":
                self.pos_counts[w] += 1; self.pos_total += 1
            elif label == "neg":
                self.neg_counts[w] += 1; self.neg_total += 1
            self.vocab.add(w)
        if label == "pos": self.pos_docs += 1
        elif label == "neg": self.neg_docs += 1

    def ready(self) -> bool:
        return self.pos_docs >= 10 and self.neg_docs >= 10 and len(self.vocab) >= 100

    def predict_llr(self, text: str) -> tuple[float, int]:
        toks = [lemma(t) for t in to_tokens(normalize_text(text))]
        if not toks: return 0.0, 0
        V = max(1, len(self.vocab))
        alpha = 1.0
        prior_pos = math.log((self.pos_docs + 1) / (self.pos_docs + self.neg_docs + 2))
        prior_neg = math.log((self.neg_docs + 1) / (self.pos_docs + self.neg_docs + 2))
        ll_pos = prior_pos
        ll_neg = prior_neg
        for w in toks:
            cp = self.pos_counts.get(w, 0)
            cn = self.neg_counts.get(w, 0)
            ll_pos += math.log((cp + alpha) / (self.pos_total + alpha * V))
            ll_neg += math.log((cn + alpha) / (self.neg_total + alpha * V))
        return (ll_pos - ll_neg), len(toks)

def train_nb_from_rows(rows):
    nb = NBModel()
    for r in rows:
        txt = (r.get(TEXT_COL) or "").strip()
        rating_raw = r.get(RATING_COL)
        if not txt or rating_raw in (None, ""):
            continue
        try:
            rating = float(str(rating_raw).replace(",", "."))
        except:
            continue
        if rating >= 4.0:
            nb.fit_doc(txt, "pos")
        elif rating <= 2.0:
            nb.fit_doc(txt, "neg")
    return nb

# ---------- –∞–Ω—Å–∞–º–±–ª—å —Å —è–≤–Ω–æ–π ¬´–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–π –∑–æ–Ω–æ–π¬ª ----------
def ensemble_label(text: str, nb: NBModel) -> str:
    # —Ç—Ä–∏–≥–≥–µ—Ä—ã ‚Äî –∂—ë—Å—Ç–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
    hard_neg, hard_pos, ph_score = phrase_flags_and_score(text)
    if hard_neg: return "negative"
    if hard_pos: return "positive"

    # –ª–µ–∫—Å–∏–∫–∞
    ls, ln, pos_hits, neg_hits = lex_score(text)
    lex_norm = (ls + ph_score) / max(3.0, float(ln))
    lex_comp = math.tanh(lex_norm)  # [-1,1]
    lex_cls = lex_label(ls + ph_score, ln)

    # –µ—Å–ª–∏ NB –Ω–µ –≥–æ—Ç–æ–≤ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —á–∏—Å—Ç–æ –ª–µ–∫—Å–∏–∫—É
    if not (nb and nb.ready()):
        return lex_cls

    # NB
    llr, n = nb.predict_llr(text)
    nb_norm = llr / max(3.0, float(n))
    nb_comp = math.tanh(nb_norm)

    # –∞–Ω—Å–∞–º–±–ª—å
    fused = 0.6 * nb_comp + 0.4 * lex_comp

    # --- –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è ---
    # 1) —É–∑–∫–∞—è ¬´–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –∑–æ–Ω–∞¬ª –≤–æ–∫—Ä—É–≥ –Ω—É–ª—è
    if abs(fused) < 0.08:
        return "neutral"

    # 2) –º–æ–¥–µ–ª–∏ —Å–ø–æ—Ä—è—Ç, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è ‚Äî –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ
    if (lex_comp * nb_comp) < 0 and max(abs(lex_comp), abs(nb_comp)) < 0.22:
        return "neutral"

    # 3) –º–∞–ª–æ —è–≤–Ω—ã—Ö –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –ø–æ–ø–∞–¥–∞–Ω–∏–π –∏ —Å–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª
    if (pos_hits + neg_hits) <= 1 and abs(fused) < 0.15:
        return "neutral"

    # --- —è–≤–Ω—ã–µ –∫–ª–∞—Å—Å—ã ---
    if fused >= 0.15:
        return "positive"
    if fused <= -0.13:
        return "negative"

    # –µ—Å–ª–∏ –≤—Å—ë –µ—â—ë –Ω–∞ –≥—Ä–∞–Ω–∏ ‚Äî –æ—Ç–¥–∞–¥–∏–º ¬´–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ¬ª
    return "neutral"

# ---------- CSV –æ–±—Ä–∞–±–æ—Ç–∫–∞ ----------
def process_csv(path: Path):
    with path.open("r", encoding="utf-8-sig", newline="") as f:
        rdr = csv.DictReader(f)
        rows = list(rdr)
        fieldnames = list(rdr.fieldnames or [])

    if SENT_COL not in fieldnames:
        fieldnames.append(SENT_COL)

    nb = train_nb_from_rows(rows)

    for r in rows:
        text = (r.get(TEXT_COL) or "").strip()
        r[SENT_COL] = ensemble_label(text, nb)

    with path.open("w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
        w.writeheader()
        w.writerows(rows)

    print(f"‚úì –æ–±–Ω–æ–≤–ª—ë–Ω: {path}  ({len(rows)} —Å—Ç—Ä–æ–∫)  "
          f"{'(RuSentiLex: –ª–æ–∫–∞–ª—å–Ω—ã–π)' if RU_SENTI else '(–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å)'}  "
          f"{'(NB: –æ–±—É—á–µ–Ω)' if nb.ready() else '(NB: –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ª–µ–∫—Å–∏–∫–∞)'}")

if __name__ == "__main__":
    p = Path(sys.argv[1]) if len(sys.argv) > 1 else Path(DEFAULT_CSV)
    process_csv(p)
